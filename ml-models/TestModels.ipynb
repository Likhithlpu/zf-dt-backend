{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_excel('/Users/likhithkanigolla/IIITH/code-files/Digital-Twin/ZF/Soil_test/Soil Data.xlsx')\n",
    "\n",
    "# Handle missing values (if any)\n",
    "# data.fillna(method='ffill', inplace=True) # Example method to fill missing values\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['entry_id', 'tdsValue', 'Voltage','tdsValue_without_temp'])\n",
    "y = data['tdsValue']\n",
    "\n",
    "# Encode categorical variables if necessary\n",
    "# X = pd.get_dummies(X, drop_first=True) # Example for one-hot encoding\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - MAE: 10.291481541374573, MSE: 1096.150488183664, R²: 0.34554971002965584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f'Linear Regression - MAE: {lr_mae}, MSE: {lr_mse}, R²: {lr_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - MAE: 10.016005797262086, MSE: 1043.0483177500867, R²: 0.377254052830155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train the model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Random Forest - MAE: {rf_mae}, MSE: {rf_mse}, R²: {rf_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - MAE: 9.6912412581883, MSE: 1102.4847677608416, R²: 0.34176786515459123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train the model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "gb_mae = mean_absolute_error(y_test, y_pred_gb)\n",
    "gb_mse = mean_squared_error(y_test, y_pred_gb)\n",
    "gb_r2 = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f'Gradient Boosting - MAE: {gb_mae}, MSE: {gb_mse}, R²: {gb_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regressor - MAE: 9.130894346435898, MSE: 1090.042209654508, R²: 0.3491966222900791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Train the model\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "svr_mae = mean_absolute_error(y_test, y_pred_svr)\n",
    "svr_mse = mean_squared_error(y_test, y_pred_svr)\n",
    "svr_r2 = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f'Support Vector Regressor - MAE: {svr_mae}, MSE: {svr_mse}, R²: {svr_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network - MAE: 76.52659275443665, MSE: 7720.153421776161, R²: -3.609272814236429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Train the model\n",
    "nn_model = MLPRegressor(random_state=42, max_iter=200)\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "nn_mae = mean_absolute_error(y_test, y_pred_nn)\n",
    "nn_mse = mean_squared_error(y_test, y_pred_nn)\n",
    "nn_r2 = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f'Neural Network - MAE: {nn_mae}, MSE: {nn_mse}, R²: {nn_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - MAE: 9.913790434120974, MSE: 995.336952276266, R²: 0.40573984680262576\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=200)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "xgb_mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "xgb_mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "xgb_r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'XGBoost - MAE: {xgb_mae}, MSE: {xgb_mse}, R²: {xgb_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 82\n",
      "[LightGBM] [Info] Number of data points in the train set: 1300, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 341.548041\n",
      "LightGBM - MAE: 10.082362792070422, MSE: 1082.3796628939037, R²: 0.3537715013814511\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Train the model\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "lgb_mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "lgb_mse = mean_squared_error(y_test, y_pred_lgb)\n",
    "lgb_r2 = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f'LightGBM - MAE: {lgb_mae}, MSE: {lgb_mse}, R²: {lgb_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 82\n",
      "[LightGBM] [Info] Number of data points in the train set: 1300, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 341.548041\n",
      "                                MAE          MSE        R²\n",
      "XGBoost                    9.913790   995.336952  0.405740\n",
      "Random Forest             10.016006  1043.048318  0.377254\n",
      "LightGBM                  10.082363  1082.379663  0.353772\n",
      "Support Vector Regressor   9.130894  1090.042210  0.349197\n",
      "Linear Regression         10.291482  1096.150488  0.345550\n",
      "Gradient Boosting          9.691241  1102.484768  0.341768\n",
      "Neural Network            76.526593  7720.153422 -3.609273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Assuming you have preprocessed your data as shown earlier\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Support Vector Regressor': SVR(),\n",
    "    'Neural Network': MLPRegressor(random_state=42, max_iter=200),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42, n_estimators=200),\n",
    "    'LightGBM': lgb.LGBMRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Sort the results by R² score\n",
    "results_df = results_df.sort_values(by='R²', ascending=False)\n",
    "\n",
    "# Save the results to a CSV file for sharing\n",
    "results_df.to_csv('model_results.csv')\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynominal Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree 1) - MAE: 10.291481541374573, MSE: 1096.1504881836636, R²: 0.34554971002965607\n",
      "Polynomial Regression (degree 2) - MAE: 9.518190104006903, MSE: 1084.9482317941583, R²: 0.352237952220363\n",
      "Polynomial Regression (degree 3) - MAE: 9.603115983512268, MSE: 1101.6987526049156, R²: 0.3422371509435974\n",
      "Polynomial Regression (degree 4) - MAE: 10.855994647239264, MSE: 1127.3631720627259, R²: 0.3269143581910112\n"
     ]
    }
   ],
   "source": [
    "# Define the degree of the polynomial\n",
    "degree = 1  # You can experiment with different degrees\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly_train = poly.fit_transform(X_train_scaled)\n",
    "X_poly_test = poly.transform(X_test_scaled)\n",
    "\n",
    "# Fit a Linear Regression model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly = poly_model.predict(X_poly_test)\n",
    "\n",
    "# Evaluate the model\n",
    "poly_mae = mean_absolute_error(y_test, y_pred_poly)\n",
    "poly_mse = mean_squared_error(y_test, y_pred_poly)\n",
    "poly_r2 = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f'Polynomial Regression (degree {degree}) - MAE: {poly_mae}, MSE: {poly_mse}, R²: {poly_r2}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree 2) - MAE: 9.518190104006903, MSE: 1084.9482317941583, R²: 0.352237952220363\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression with degree 2\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly_train = poly.fit_transform(X_train_scaled)\n",
    "X_poly_test = poly.transform(X_test_scaled)\n",
    "\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly_train, y_train)\n",
    "y_pred_poly = poly_model.predict(X_poly_test)\n",
    "\n",
    "poly_mae = mean_absolute_error(y_test, y_pred_poly)\n",
    "poly_mse = mean_squared_error(y_test, y_pred_poly)\n",
    "poly_r2 = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f'Polynomial Regression (degree {degree}) - MAE: {poly_mae}, MSE: {poly_mse}, R²: {poly_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression with degree 3\n",
    "degree = 3\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly_train = poly.fit_transform(X_train_scaled)\n",
    "X_poly_test = poly.transform(X_test_scaled)\n",
    "\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly_train, y_train)\n",
    "y_pred_poly = poly_model.predict(X_poly_test)\n",
    "\n",
    "poly_mae = mean_absolute_error(y_test, y_pred_poly)\n",
    "poly_mse = mean_squared_error(y_test, y_pred_poly)\n",
    "poly_r2 = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f'Polynomial Regression (degree {degree}) - MAE: {poly_mae}, MSE: {poly_mse}, R²: {poly_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression with degree 4\n",
    "degree = 4\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly_train = poly.fit_transform(X_train_scaled)\n",
    "X_poly_test = poly.transform(X_test_scaled)\n",
    "\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly_train, y_train)\n",
    "y_pred_poly = poly_model.predict(X_poly_test)\n",
    "\n",
    "poly_mae = mean_absolute_error(y_test, y_pred_poly)\n",
    "poly_mse = mean_squared_error(y_test, y_pred_poly)\n",
    "poly_r2 = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f'Polynomial Regression (degree {degree}) - MAE: {poly_mae}, MSE: {poly_mse}, R²: {poly_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS Value: 350.29 (mg/L)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the input array\n",
    "input_data = np.array([[27.125, 2, 200]])\n",
    "\n",
    "input_features_normalized = scaler.transform(input_data)\n",
    "# print(input_features_normalized)\n",
    "\n",
    "# Transform the input data using polynomial features\n",
    "input_data_poly = poly.transform(input_features_normalized)\n",
    "# print(input_data_poly)\n",
    "\n",
    "# Predict using the polynomial regression model\n",
    "prediction = poly_model.predict(input_data_poly)\n",
    "print(f'TDS Value: {prediction[0]:.2f} (mg/L)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 2\n",
      "Polynomial Regression Equation:\n",
      "y = 832900019706.9500 + (0.0000) * 1 + (1.4046) * x1 + (-48713432252.4062) * x2 + (20.7860) * x3 + (-2.9069) * x1^2 + (4.7718) * x1 x2 + (-3.6989) * x1 x3 + (-832900019366.2006) * x2^2 + (0.6818) * x2 x3 + (-0.9937) * x3^2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define the polynomial features object\n",
    "print(\"Degree:\",degree)\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = poly.get_feature_names_out(input_features=['x1', 'x2', 'x3'])\n",
    "\n",
    "# Rest of the code remains the same\n",
    "intercept = poly_model.intercept_\n",
    "coefficients = poly_model.coef_\n",
    "equation = f\"y = {intercept:.4f}\"\n",
    "for coef, feature in zip(coefficients, feature_names):\n",
    "    equation += f\" + ({coef:.4f}) * {feature}\"\n",
    "print(\"Polynomial Regression Equation:\")\n",
    "print(equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Testing(Optinoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74913464  0.97118423 -0.28561575]\n",
      "0.7491346376514885 0.9711842277268257 -0.2856157453678386\n",
      "350.2948796148556\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[27.125, 2, 200]])\n",
    "\n",
    "print(input_features_normalized[0])\n",
    "x1=input_features_normalized[0][0]\n",
    "x2=input_features_normalized[0][1]\n",
    "x3=input_features_normalized[0][2]\n",
    "print(x1,x2,x3)\n",
    "result=832900019706.9500 + (0.0000) * 1 + (1.4046) * x1 + (-48713432252.4062) * x2 + (20.7860) * x3 + (-2.9069) * (x1*x1) + (4.7718) * (x1*x2) + (-3.6989) * (x1*x3) + (-832900019366.2006) * (x2*x2) + (0.6818) * (x2*x3) + (-0.9937) * (x3*x3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Output/soil_XGBoost.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming your XGBoost model is stored in the variable 'xgboost_model'\n",
    "model =  xgb.XGBRegressor(random_state=42, n_estimators=200)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Define the file path for saving the model\n",
    "file_path = 'Output/soil_XGBoost.pkl'\n",
    "\n",
    "# Save the model to the file\n",
    "joblib.dump(model, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74913464  0.97118423 -0.28561575]]\n",
      "[343.0992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = joblib.load('/Users/likhithkanigolla/IIITH/code-files/Digital-Twin/ZF/backend/ml-models/Output/soil_XGBoost.pkl')\n",
    "\n",
    "# Prepare the input data for prediction\n",
    "input_data = [27.125,2,200]\n",
    "input_features = np.array(input_data).reshape(1, -1)\n",
    "\n",
    "input_features_normalized = scaler.transform(input_features)\n",
    "print(input_features_normalized)\n",
    "# Make predictions\n",
    "predictions = model.predict(input_features_normalized)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
